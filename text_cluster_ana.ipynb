{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import cluster_paragraphs\n",
    "from vectorizer import vectorize_text\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando dados dos países a serem avaliados\n",
    "df = pd.read_csv('/home/raime/Documentos/Repositórios/text_clustering/latam.csv')\n",
    "nomes_colunas = df.columns\n",
    "dados_brasil = df.loc[df[' País donde se chequeó la desinformación']== \"Brasil\"]\n",
    "dados_mexico = df.loc[df[' País donde se chequeó la desinformación']== \"México\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# criação de 2 listas de notícias, uma para o brasil e outra para o mexico\n",
    " \n",
    "noticias_brasil = dados_brasil['Desinformación o afirmación chequeada (o título del explicador)']\n",
    "\n",
    "lista_de_noticias_br = []\n",
    "\n",
    "for noticia in noticias_brasil:\n",
    "    lista_de_noticias_br.append(noticia)\n",
    "\n",
    "type(lista_de_noticias_br)\n",
    "# lista de noticias_br extraídas do banco de dados.\n",
    "\n",
    "noticias_mexico = dados_mexico['Desinformación o afirmación chequeada (o título del explicador)']\n",
    "\n",
    "lista_de_noticias_me = []\n",
    "\n",
    "for news in noticias_mexico:\n",
    "    lista_de_noticias_me.append(news)\n",
    "\n",
    "type(lista_de_noticias_me)\n",
    "\n",
    "#lista_de_noticias_me extraídas do banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vetorizando as noticias do brasil e do mexico\n",
    "vetores_br = []\n",
    "for i in lista_de_noticias_br:\n",
    "    vetores_br.append(vectorize_text(i))\n",
    "\n",
    "vetores_me = []\n",
    "for t in lista_de_noticias_me:\n",
    "    vetores_me.append(vectorize_text(t))\n",
    "\n",
    "vetores_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparo de dados para contagem de palavras mais comuns\n",
    "bag_of_words_br = []\n",
    "for vetores in vetores_br:\n",
    "    for palavras in vetores:\n",
    "        bag_of_words_br.append(palavras)\n",
    "\n",
    "bag_of_words_me = []\n",
    "for vectores in vetores_me:\n",
    "    for palabras in vectores:\n",
    "        bag_of_words_me.append(palabras)\n",
    "\n",
    "bag_of_words_br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords em espanhol: fonte nltk.com\n",
    "from nltk.corpus import stopwords\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "print(spanish_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo stopwords\n",
    "stopword = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "def remove_stopwords(bag_of_words_br):\n",
    "    bag_of_words_br = [word for word in bag_of_words_br if word not in stopword]\n",
    "    return bag_of_words_br\n",
    "\n",
    "bag_of_words_clean_br = remove_stopwords(bag_of_words_br) # brasil\n",
    "\n",
    "def remove_stoppalabras(bag_of_words_me):\n",
    "    bag_of_words_me = [word for word in bag_of_words_me if word not in stopword]\n",
    "    return bag_of_words_me\n",
    "\n",
    "bag_of_words_clean_me = remove_stoppalabras(bag_of_words_me) # mexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contagem das palavras mais comuuns com contagem\n",
    "from collections import Counter\n",
    "b = Counter(bag_of_words_clean_br)\n",
    "palavras_comuns_br = b.most_common(50)\n",
    "\n",
    "m = Counter(bag_of_words_clean_me)\n",
    "palavras_comuns_me = m.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando tabela de palavras mais comuuns do brasil e do mexico\n",
    "brasil_tabela_palavras_comuns = pd.DataFrame((palavras_comuns_br))\n",
    "palavras_brasileiras = brasil_tabela_palavras_comuns.rename(columns={0:'Palavras mais comuns', 1:'Quantas vezes apareceu'})\n",
    "palavras_brasileiras.to_excel('PalavrasBrasileirasComuns.xlsx')\n",
    "\n",
    "mexico_tabela_palavras_comuns = pd.DataFrame((palavras_comuns_me))\n",
    "palavras_mexicanas = mexico_tabela_palavras_comuns.rename(columns={0:'Palavras mais comuns', 1:'Quantas vezes apareceu'})\n",
    "palavras_mexicanas.to_excel('PalavrasMexicanasComuns.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramos na vetorização das notícias"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc0a6f70a5fe7139076d1c5a932906fedb2d4be8d39032e2b1ad3e83968f6867"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
